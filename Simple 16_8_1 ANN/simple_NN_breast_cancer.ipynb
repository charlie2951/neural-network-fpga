{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPGA Implementation of a Simple Neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Normalization and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "# split the train and test dataset\n",
    "X_train, X_test,\\\n",
    "\ty_train, y_test = train_test_split(X, y,\n",
    "\t\t\t\t\t\t\t\t\ttest_size=0.20,\n",
    "\t\t\t\t\t\t\t\t\trandom_state=23)\n",
    "#feature Scaling  \n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "st_x= StandardScaler()    \n",
    "X_train= st_x.fit_transform(X_train)    \n",
    "X_test= st_x.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Keras module of Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 17:20:59.953783: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 17:21:03.861179: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-11 17:21:03.875837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 17:21:13.272288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    " **A simple fully connected Neural network contains 16 Neuron at 1st layer with _Relu_ activation, 8 neuron in 2nd layer with _Relu_ activation, Final/output layer having one Neuron with _Sigmoid_ activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 17:21:20.736108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-11 17:21:20.740437: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5/5 [==============================] - 3s 26ms/step - loss: 0.6281 - accuracy: 0.6593\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5603 - accuracy: 0.7648\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5042 - accuracy: 0.8418\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.8615\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4142 - accuracy: 0.8945\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3781 - accuracy: 0.9077\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3463 - accuracy: 0.9121\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3175 - accuracy: 0.9165\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2921 - accuracy: 0.9253\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2688 - accuracy: 0.9363\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2477 - accuracy: 0.9429\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2289 - accuracy: 0.9429\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9473\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9560\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1818 - accuracy: 0.9604\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1696 - accuracy: 0.9648\n",
      "Epoch 17/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.9692\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1494 - accuracy: 0.9692\n",
      "Epoch 19/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1414 - accuracy: 0.9714\n",
      "Epoch 20/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9714\n",
      "Epoch 21/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1279 - accuracy: 0.9714\n",
      "Epoch 22/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1223 - accuracy: 0.9714\n",
      "Epoch 23/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9714\n",
      "Epoch 24/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1130 - accuracy: 0.9714\n",
      "Epoch 25/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1088 - accuracy: 0.9714\n",
      "Epoch 26/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9758\n",
      "Epoch 27/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9758\n",
      "Epoch 28/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0988 - accuracy: 0.9758\n",
      "Epoch 29/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0957 - accuracy: 0.9758\n",
      "Epoch 30/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0931 - accuracy: 0.9758\n",
      "Epoch 31/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0905 - accuracy: 0.9780\n",
      "Epoch 32/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0882 - accuracy: 0.9780\n",
      "Epoch 33/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9780\n",
      "Epoch 34/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0838 - accuracy: 0.9780\n",
      "Epoch 35/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9802\n",
      "Epoch 36/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0798 - accuracy: 0.9846\n",
      "Epoch 37/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9846\n",
      "Epoch 38/150\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0764 - accuracy: 0.9846\n",
      "Epoch 39/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.9868\n",
      "Epoch 40/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0730 - accuracy: 0.9868\n",
      "Epoch 41/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9846\n",
      "Epoch 42/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0700 - accuracy: 0.9868\n",
      "Epoch 43/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0687 - accuracy: 0.9868\n",
      "Epoch 44/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0674 - accuracy: 0.9868\n",
      "Epoch 45/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9868\n",
      "Epoch 46/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0649 - accuracy: 0.9868\n",
      "Epoch 47/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0638 - accuracy: 0.9890\n",
      "Epoch 48/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0625 - accuracy: 0.9890\n",
      "Epoch 49/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0615 - accuracy: 0.9890\n",
      "Epoch 50/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0604 - accuracy: 0.9890\n",
      "Epoch 51/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9890\n",
      "Epoch 52/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0584 - accuracy: 0.9890\n",
      "Epoch 53/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9890\n",
      "Epoch 54/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0566 - accuracy: 0.9890\n",
      "Epoch 55/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0557 - accuracy: 0.9890\n",
      "Epoch 56/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0549 - accuracy: 0.9890\n",
      "Epoch 57/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0540 - accuracy: 0.9890\n",
      "Epoch 58/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9890\n",
      "Epoch 59/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.9890\n",
      "Epoch 60/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9890\n",
      "Epoch 61/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9890\n",
      "Epoch 62/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 0.9890\n",
      "Epoch 63/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9890\n",
      "Epoch 64/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.9890\n",
      "Epoch 65/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9890\n",
      "Epoch 66/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9890\n",
      "Epoch 67/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0471 - accuracy: 0.9890\n",
      "Epoch 68/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0464 - accuracy: 0.9890\n",
      "Epoch 69/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0458 - accuracy: 0.9890\n",
      "Epoch 70/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.9890\n",
      "Epoch 71/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 0.9890\n",
      "Epoch 72/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.9890\n",
      "Epoch 73/150\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0436 - accuracy: 0.9890\n",
      "Epoch 74/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 0.9890\n",
      "Epoch 75/150\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0425 - accuracy: 0.9890\n",
      "Epoch 76/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9890\n",
      "Epoch 77/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9890\n",
      "Epoch 78/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9890\n",
      "Epoch 79/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.9890\n",
      "Epoch 80/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9912\n",
      "Epoch 81/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0395 - accuracy: 0.9912\n",
      "Epoch 82/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0390 - accuracy: 0.9912\n",
      "Epoch 83/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9912\n",
      "Epoch 84/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9912\n",
      "Epoch 85/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 0.9912\n",
      "Epoch 86/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9912\n",
      "Epoch 87/150\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9912\n",
      "Epoch 88/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9912\n",
      "Epoch 89/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0360 - accuracy: 0.9912\n",
      "Epoch 90/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0356 - accuracy: 0.9912\n",
      "Epoch 91/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9912\n",
      "Epoch 92/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0349 - accuracy: 0.9912\n",
      "Epoch 93/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9912\n",
      "Epoch 94/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 0.9912\n",
      "Epoch 95/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 0.9912\n",
      "Epoch 96/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 0.9912\n",
      "Epoch 97/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9912\n",
      "Epoch 98/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0327 - accuracy: 0.9912\n",
      "Epoch 99/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9912\n",
      "Epoch 100/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9912\n",
      "Epoch 101/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9912\n",
      "Epoch 102/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9912\n",
      "Epoch 103/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9912\n",
      "Epoch 104/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9912\n",
      "Epoch 105/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9912\n",
      "Epoch 106/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.9912\n",
      "Epoch 107/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9912\n",
      "Epoch 108/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.9912\n",
      "Epoch 109/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9912\n",
      "Epoch 110/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9912\n",
      "Epoch 111/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9912\n",
      "Epoch 112/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.9912\n",
      "Epoch 113/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9912\n",
      "Epoch 114/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9912\n",
      "Epoch 115/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.9912\n",
      "Epoch 116/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.9912\n",
      "Epoch 117/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9934\n",
      "Epoch 118/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.9934\n",
      "Epoch 119/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.9934\n",
      "Epoch 120/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9934\n",
      "Epoch 121/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.9934\n",
      "Epoch 122/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9934\n",
      "Epoch 123/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0252 - accuracy: 0.9934\n",
      "Epoch 124/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.9934\n",
      "Epoch 125/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0246 - accuracy: 0.9934\n",
      "Epoch 126/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.9934\n",
      "Epoch 127/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.9934\n",
      "Epoch 128/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.9934\n",
      "Epoch 129/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.9934\n",
      "Epoch 130/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0234 - accuracy: 0.9934\n",
      "Epoch 131/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.9934\n",
      "Epoch 132/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9934\n",
      "Epoch 133/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9934\n",
      "Epoch 134/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.9934\n",
      "Epoch 135/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9934\n",
      "Epoch 136/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.9934\n",
      "Epoch 137/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 0.9934\n",
      "Epoch 138/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 0.9934\n",
      "Epoch 139/150\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0213 - accuracy: 0.9934\n",
      "Epoch 140/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.9934\n",
      "Epoch 141/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9934\n",
      "Epoch 142/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 0.9934\n",
      "Epoch 143/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0205 - accuracy: 0.9934\n",
      "Epoch 144/150\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0204 - accuracy: 0.9934\n",
      "Epoch 145/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9934\n",
      "Epoch 146/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9934\n",
      "Epoch 147/150\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 0.9934\n",
      "Epoch 148/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9934\n",
      "Epoch 149/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.9934\n",
      "Epoch 150/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60d8113610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(16, activation='relu', input_shape=(30,)))\n",
    "# Adding dropout to prevent overfitting\n",
    "#classifier.add(Dropout(0.1))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(8, activation='relu'))\n",
    "# Adding dropout to prevent overfitting\n",
    "#classifier.add(Dropout(0.1))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=100, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[[38  1]\n",
      " [ 1 74]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_keras=classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 98.24561403508771 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score is:',100*accuracy_score(y_test, y_pred),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Weight and Bias of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model parameters: weight and bias\n",
    "first_layer_weights = classifier.layers[0].get_weights()[0]\n",
    "first_layer_biases  = classifier.layers[0].get_weights()[1]\n",
    "second_layer_weights = classifier.layers[1].get_weights()[0]\n",
    "second_layer_biases  = classifier.layers[1].get_weights()[1]\n",
    "third_layer_weights = classifier.layers[2].get_weights()[0]\n",
    "third_layer_biases  = classifier.layers[2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2479373   0.07805399  0.0445666   0.20560437  0.20575419 -0.11484732\n",
      "  0.07847597  0.23216642  0.00573132  0.16846716  0.04678782  0.19032331\n",
      "  0.10302778  0.16017412  0.22046196 -0.04226402]\n"
     ]
    }
   ],
   "source": [
    "print(first_layer_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch weight and bias of each layer of trained model and store them into Numpy array, then multiply Quantization factor $2^{12}$ and after rounding, save them as a separate C file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#export model parameters for 1st layer\n",
    "layer0_w = first_layer_weights\n",
    "layer0_b = first_layer_biases\n",
    "n_features = 30 #no of features as input\n",
    "n_layer0_neuron = 16 #no of neuron in layer0\n",
    "layer0_w = pow(2,12)*(layer0_w.T)\n",
    "layer0_b = pow(2,12)*(layer0_b)\n",
    "f=open('model_params_layer0.c','w')\n",
    "#write weights of layer_0 into test file\n",
    "f.write(\"int w0[16][30]= {\")\n",
    "for j in range(n_layer0_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer0_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\")\n",
    "    if(j<n_layer0_neuron -1 ):\n",
    "        #print(j)\n",
    "        f.write(\",\")\n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b0[16] = {\")\n",
    "for i in range(n_layer0_neuron):\n",
    "    f.write(str(round(layer0_b[i])))\n",
    "    if(i< n_layer0_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "\n",
    "#export model parameters for 2nd layer\n",
    "layer1_w = second_layer_weights\n",
    "layer1_b = second_layer_biases\n",
    "n_features = n_layer0_neuron #no of features as input i.e o/p of prev layer\n",
    "n_layer1_neuron = 8 #no of neuron in layer0\n",
    "layer1_w = pow(2,12)*(layer1_w.T)\n",
    "layer1_b = pow(2,12)*(layer1_b)\n",
    "f=open('model_params_layer1.c','w')\n",
    "#write weights of layer_1 into test file\n",
    "f.write(\"int w1[8][16]= {\")\n",
    "for j in range(n_layer1_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer1_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\")\n",
    "    if(j<n_layer1_neuron -1):\n",
    "        #print(j)\n",
    "        f.write(\",\")\n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b1[8] = {\")\n",
    "for i in range(n_layer1_neuron):\n",
    "    f.write(str(round(layer1_b[i])))\n",
    "    if(i< n_layer1_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "\n",
    "#export model parameters for 3rd layer\n",
    "layer2_w = third_layer_weights\n",
    "layer2_b = third_layer_biases\n",
    "n_features = n_layer1_neuron #no of features as input i.e o/p of prev layer\n",
    "n_layer2_neuron = 1 #no of neuron in layer0\n",
    "layer2_w = pow(2,12)*(layer2_w.T)\n",
    "layer2_b = pow(2,12)*(layer2_b)\n",
    "f=open('model_params_layer2.c','w')\n",
    "#write weights of layer_2 into test file\n",
    "f.write(\"int w2[8]= \")\n",
    "for j in range(n_layer2_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer2_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b2 = \")\n",
    "for i in range(n_layer2_neuron):\n",
    "    f.write(str(round(layer2_b[i])))\n",
    "    if(i< n_layer2_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\";\"+\"\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly save any one test feaure into _feature.c_ file for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label for this input feature: 1\n"
     ]
    }
   ],
   "source": [
    "#Convert test features to fixed point for verification\n",
    "f = open(\"feature.c\",\"w\")\n",
    "f.write(\"int x[30] = {\")\n",
    "for i in range(30):\n",
    "    f.write(str(round((pow(2,12)*X_test[0][i]))))\n",
    "    if(i<29):\n",
    "            f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "print(\"Actual label for this input feature:\",y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the current working directory where this script is located. Few C source code file is generated. They contain weight and bias values of different layer. Now create a C code to implement the same network in fixed point representation and include those source files into that project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
