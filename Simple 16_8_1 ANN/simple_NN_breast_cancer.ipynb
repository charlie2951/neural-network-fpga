{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPGA Implementation of a Simple Neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Normalization and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "# split the train and test dataset\n",
    "X_train, X_test,\\\n",
    "\ty_train, y_test = train_test_split(X, y,\n",
    "\t\t\t\t\t\t\t\t\ttest_size=0.20,\n",
    "\t\t\t\t\t\t\t\t\trandom_state=23)\n",
    "#feature Scaling  \n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "st_x= StandardScaler()    \n",
    "X_train= st_x.fit_transform(X_train)    \n",
    "X_test= st_x.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Keras module of Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    " **A simple fully connected Neural network contains 16 Neuron at 1st layer with _Relu_ activation, 8 neuron in 2nd layer with _Relu_ activation, Final/output layer having one Neuron with _Sigmoid_ activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6462\n",
      "Epoch 2/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.6747\n",
      "Epoch 3/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7253\n",
      "Epoch 4/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7626\n",
      "Epoch 5/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.8066\n",
      "Epoch 6/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.8418\n",
      "Epoch 7/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8615\n",
      "Epoch 8/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8791\n",
      "Epoch 9/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8901\n",
      "Epoch 10/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.9055\n",
      "Epoch 11/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.9121\n",
      "Epoch 12/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.9143\n",
      "Epoch 13/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9209\n",
      "Epoch 14/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2708 - accuracy: 0.9275\n",
      "Epoch 15/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.9275\n",
      "Epoch 16/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9319\n",
      "Epoch 17/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9341\n",
      "Epoch 18/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2124 - accuracy: 0.9363\n",
      "Epoch 19/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9429\n",
      "Epoch 20/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9451\n",
      "Epoch 21/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9473\n",
      "Epoch 22/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9495\n",
      "Epoch 23/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9473\n",
      "Epoch 24/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1589 - accuracy: 0.9516\n",
      "Epoch 25/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.9560\n",
      "Epoch 26/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.9604\n",
      "Epoch 27/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9626\n",
      "Epoch 28/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9648\n",
      "Epoch 29/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.9648\n",
      "Epoch 30/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1280 - accuracy: 0.9670\n",
      "Epoch 31/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9692\n",
      "Epoch 32/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9692\n",
      "Epoch 33/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9780\n",
      "Epoch 34/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9780\n",
      "Epoch 35/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.9780\n",
      "Epoch 36/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9780\n",
      "Epoch 37/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9802\n",
      "Epoch 38/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1014 - accuracy: 0.9802\n",
      "Epoch 39/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9802\n",
      "Epoch 40/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9802\n",
      "Epoch 41/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9802\n",
      "Epoch 42/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9802\n",
      "Epoch 43/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9802\n",
      "Epoch 44/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9802\n",
      "Epoch 45/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0856 - accuracy: 0.9802\n",
      "Epoch 46/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9802\n",
      "Epoch 47/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9802\n",
      "Epoch 48/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9824\n",
      "Epoch 49/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9824\n",
      "Epoch 50/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9824\n",
      "Epoch 51/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9824\n",
      "Epoch 52/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9824\n",
      "Epoch 53/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9824\n",
      "Epoch 54/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0715 - accuracy: 0.9824\n",
      "Epoch 55/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9824\n",
      "Epoch 56/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9846\n",
      "Epoch 57/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9868\n",
      "Epoch 58/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9868\n",
      "Epoch 59/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9868\n",
      "Epoch 60/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9868\n",
      "Epoch 61/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0642 - accuracy: 0.9868\n",
      "Epoch 62/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9890\n",
      "Epoch 63/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9890\n",
      "Epoch 64/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9890\n",
      "Epoch 65/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9890\n",
      "Epoch 66/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9890\n",
      "Epoch 67/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9890\n",
      "Epoch 68/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9890\n",
      "Epoch 69/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9890\n",
      "Epoch 70/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9890\n",
      "Epoch 71/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9912\n",
      "Epoch 72/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9912\n",
      "Epoch 73/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9912\n",
      "Epoch 74/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9912\n",
      "Epoch 75/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9912\n",
      "Epoch 76/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9912\n",
      "Epoch 77/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9912\n",
      "Epoch 78/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9912\n",
      "Epoch 79/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9912\n",
      "Epoch 80/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9912\n",
      "Epoch 81/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9912\n",
      "Epoch 82/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9912\n",
      "Epoch 83/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9912\n",
      "Epoch 84/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9912\n",
      "Epoch 85/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9912\n",
      "Epoch 86/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9912\n",
      "Epoch 87/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9912\n",
      "Epoch 88/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9912\n",
      "Epoch 89/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0458 - accuracy: 0.9912\n",
      "Epoch 90/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9912\n",
      "Epoch 91/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9912\n",
      "Epoch 92/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.9912\n",
      "Epoch 93/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.9912\n",
      "Epoch 94/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9912\n",
      "Epoch 95/150\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.9912\n",
      "Epoch 96/150\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9912\n",
      "Epoch 97/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.9912\n",
      "Epoch 98/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9912\n",
      "Epoch 99/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9912\n",
      "Epoch 100/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9912\n",
      "Epoch 101/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9912\n",
      "Epoch 102/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9912\n",
      "Epoch 103/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9912\n",
      "Epoch 104/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9912\n",
      "Epoch 105/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9912\n",
      "Epoch 106/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.9912\n",
      "Epoch 107/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9912\n",
      "Epoch 108/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9912\n",
      "Epoch 109/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9912\n",
      "Epoch 110/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.9912\n",
      "Epoch 111/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9912\n",
      "Epoch 112/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0368 - accuracy: 0.9912\n",
      "Epoch 113/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9912\n",
      "Epoch 114/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9912\n",
      "Epoch 115/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9912\n",
      "Epoch 116/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9912\n",
      "Epoch 117/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9912\n",
      "Epoch 118/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9912\n",
      "Epoch 119/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9912\n",
      "Epoch 120/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9912\n",
      "Epoch 121/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9912\n",
      "Epoch 122/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9912\n",
      "Epoch 123/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9934\n",
      "Epoch 124/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9934\n",
      "Epoch 125/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9934\n",
      "Epoch 126/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 0.9934\n",
      "Epoch 127/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9934\n",
      "Epoch 128/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.9934\n",
      "Epoch 129/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9934\n",
      "Epoch 130/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.9934\n",
      "Epoch 131/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9934\n",
      "Epoch 132/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9934\n",
      "Epoch 133/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9934\n",
      "Epoch 134/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9934\n",
      "Epoch 135/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9934\n",
      "Epoch 136/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9934\n",
      "Epoch 137/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9934\n",
      "Epoch 138/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9934\n",
      "Epoch 139/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9934\n",
      "Epoch 140/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9934\n",
      "Epoch 141/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9934\n",
      "Epoch 142/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9934\n",
      "Epoch 143/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9934\n",
      "Epoch 144/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9934\n",
      "Epoch 145/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.9934\n",
      "Epoch 146/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9934\n",
      "Epoch 147/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9934\n",
      "Epoch 148/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9934\n",
      "Epoch 149/150\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9934\n",
      "Epoch 150/150\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x184f579f890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(16, activation='relu', input_shape=(30,)))\n",
    "# Adding dropout to prevent overfitting\n",
    "#classifier.add(Dropout(0.1))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(8, activation='relu'))\n",
    "# Adding dropout to prevent overfitting\n",
    "#classifier.add(Dropout(0.1))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=100, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "[[38  1]\n",
      " [ 1 74]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_keras=classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 98.24561403508771 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score is:',100*accuracy_score(y_test, y_pred),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Weight and Bias of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model parameters: weight and bias\n",
    "first_layer_weights = classifier.layers[0].get_weights()[0]\n",
    "first_layer_biases  = classifier.layers[0].get_weights()[1]\n",
    "second_layer_weights = classifier.layers[1].get_weights()[0]\n",
    "second_layer_biases  = classifier.layers[1].get_weights()[1]\n",
    "third_layer_weights = classifier.layers[2].get_weights()[0]\n",
    "third_layer_biases  = classifier.layers[2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22651652 -0.00651723  0.16685447  0.27603388  0.2276782   0.3001223\n",
      " -0.14324668  0.23136708  0.22145116  0.10626026  0.23359351  0.0282598\n",
      "  0.2324835   0.26536405  0.03963194  0.24475598]\n"
     ]
    }
   ],
   "source": [
    "print(first_layer_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch weight and bias of each layer of trained model and store them into Numpy array, then multiply Quantization factor $2^{12}$ and after rounding, save them as a separate C file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#export model parameters for 1st layer\n",
    "layer0_w = first_layer_weights\n",
    "layer0_b = first_layer_biases\n",
    "n_features = 30 #no of features as input\n",
    "n_layer0_neuron = 16 #no of neuron in layer0\n",
    "layer0_w = pow(2,12)*(layer0_w.T)\n",
    "layer0_b = pow(2,12)*(layer0_b)\n",
    "f=open('model_params_layer0.c','w')\n",
    "#write weights of layer_0 into test file\n",
    "f.write(\"int w0[16][30]= {\")\n",
    "for j in range(n_layer0_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer0_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\")\n",
    "    if(j<n_layer0_neuron -1 ):\n",
    "        #print(j)\n",
    "        f.write(\",\")\n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b0[16] = {\")\n",
    "for i in range(n_layer0_neuron):\n",
    "    f.write(str(round(layer0_b[i])))\n",
    "    if(i< n_layer0_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "\n",
    "#export model parameters for 2nd layer\n",
    "layer1_w = second_layer_weights\n",
    "layer1_b = second_layer_biases\n",
    "n_features = n_layer0_neuron #no of features as input i.e o/p of prev layer\n",
    "n_layer1_neuron = 8 #no of neuron in layer0\n",
    "layer1_w = pow(2,12)*(layer1_w.T)\n",
    "layer1_b = pow(2,12)*(layer1_b)\n",
    "f=open('model_params_layer1.c','w')\n",
    "#write weights of layer_1 into test file\n",
    "f.write(\"int w1[8][16]= {\")\n",
    "for j in range(n_layer1_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer1_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    f.write(\"}\")\n",
    "    if(j<n_layer1_neuron -1):\n",
    "        #print(j)\n",
    "        f.write(\",\")\n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b1[8] = {\")\n",
    "for i in range(n_layer1_neuron):\n",
    "    f.write(str(round(layer1_b[i])))\n",
    "    if(i< n_layer1_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "\n",
    "#export model parameters for 3rd layer\n",
    "layer2_w = third_layer_weights\n",
    "layer2_b = third_layer_biases\n",
    "n_features = n_layer1_neuron #no of features as input i.e o/p of prev layer\n",
    "n_layer2_neuron = 1 #no of neuron in layer0\n",
    "layer2_w = pow(2,12)*(layer2_w.T)\n",
    "layer2_b = pow(2,12)*(layer2_b)\n",
    "f=open('model_params_layer2.c','w')\n",
    "#write weights of layer_2 into test file\n",
    "f.write(\"int w2[8]= \")\n",
    "for j in range(n_layer2_neuron):\n",
    "    f.write(\"{\")\n",
    "    for i in range(n_features):\n",
    "        f.write(str(round(layer2_w[j][i])))\n",
    "        if(i<n_features - 1):\n",
    "            f.write(\",\")\n",
    "        if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "#write bias values into file\n",
    "f.write(\"int b2 = \")\n",
    "for i in range(n_layer2_neuron):\n",
    "    f.write(str(round(layer2_b[i])))\n",
    "    if(i< n_layer2_neuron -1):\n",
    "        f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "        f.write(\"\\n\")\n",
    "   \n",
    "f.write(\";\"+\"\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly save any one test feaure into _feature.c_ file for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual label for this input feature: 1\n"
     ]
    }
   ],
   "source": [
    "#Convert test features to fixed point for verification\n",
    "f = open(\"feature.c\",\"w\")\n",
    "f.write(\"int x[30] = {\")\n",
    "for i in range(30):\n",
    "    f.write(str(round((pow(2,12)*X_test[0][i]))))\n",
    "    if(i<29):\n",
    "            f.write(\",\")\n",
    "    if((i+1)%10==0):\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "f.write(\"};\"+\"\\n\\n\")\n",
    "f.close()\n",
    "print(\"Actual label for this input feature:\",y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the current working directory where this script is located. Few C source code file is generated. They contain weight and bias values of different layer. Now create a C code to implement the same network in fixed point representation and include those source files into that project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
